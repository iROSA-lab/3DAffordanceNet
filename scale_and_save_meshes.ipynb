{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 152 objects\n",
      "Num of semantic classes:  7\n",
      "Semantic classes:  ['Knife', 'Bag', 'Bottle', 'Scissors', 'Mug', 'Earphone', 'Hat']\n",
      "Num of affordances:  7\n",
      "All affordances:  ['handover', 'cut', 'stab', 'lift', 'wrap', 'pour', 'wear']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import join as opj\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import open3d as o3d\n",
    "from open3d.web_visualizer import draw\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import *\n",
    "from dataset import *\n",
    "\n",
    "import pickle as pkl\n",
    "data_root = 'data_root/3DGraspAff'\n",
    "train_or_test = 'train' # 'train' or 'test'\n",
    "aff_path = opj(data_root, 'anntd_remapped_full_shape_'+train_or_test+'_data.pkl')\n",
    "with open(aff_path, 'rb') as f:\n",
    "    aff_dataset = pkl.load(f)\n",
    "print(\"Loaded dataset with {} objects\".format(aff_dataset['num_objects']))\n",
    "print(\"Num of semantic classes: \", len(aff_dataset[\"semantic_classes\"]))\n",
    "print(\"Semantic classes: \", aff_dataset[\"semantic_classes\"])\n",
    "print(\"Num of affordances: \", len(aff_dataset[\"affordance_classes\"]))\n",
    "print(\"All affordances: \", aff_dataset[\"affordance_classes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Make mesh watertight if it isn't already\n",
    "import point_cloud_utils as pcu\n",
    "# import trimesh\n",
    "\n",
    "def make_watertight_pcu(mesh_vertices, mesh_faces, resolution=20_000):\n",
    "    # v, f = pcu.load_mesh_vf(mesh_path)\n",
    "    v, f = mesh_vertices, mesh_faces\n",
    "    v_watertight, f_watertight = pcu.make_mesh_watertight(v, f, resolution=resolution)\n",
    "    target_num_faces = f_watertight.shape[0] // 10  # Downsample by a factor of 10\n",
    "    v_decimate, f_decimate, v_correspondence, f_correspondence = pcu.decimate_triangle_mesh(v_watertight, f_watertight, target_num_faces)\n",
    "    # mesh_decimated = trimesh.Trimesh(vertices=v_decimate, faces=f_decimate)\n",
    "    mesh_decimated = o3d.geometry.TriangleMesh()\n",
    "    mesh_decimated.vertices = o3d.utility.Vector3dVector(v_decimate)\n",
    "    mesh_decimated.triangles = o3d.utility.Vector3iVector(f_decimate)\n",
    "    return mesh_decimated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing shape_id:  f4897c9b20e811f1309c3ff4f31971fa  for class:  Scissors\n",
      "ShapeNet category not found, try PartNet?\n",
      "Processing shape_id:  ec46d0cc7ac00f16cb72f5e3f9027736  for class:  Scissors\n",
      "ShapeNet category not found, try PartNet?\n",
      "Processing shape_id:  8b2f6413-125c-46fd-9b9b-1d2f10f6c6b5  for class:  Scissors\n",
      "ShapeNet category not found, try PartNet?\n",
      "Processing shape_id:  efd43cdfcde9eae11152189a5eecbfbb  for class:  Scissors\n",
      "ShapeNet category not found, try PartNet?\n",
      "Processing shape_id:  1dfea364-ded5-4c4d-b4d8-bf2c3c268f00  for class:  Scissors\n",
      "ShapeNet category not found, try PartNet?\n",
      "Processing shape_id:  7a7eb180-467e-4069-913a-05e395627a03  for class:  Scissors\n",
      "ShapeNet category not found, try PartNet?\n"
     ]
    }
   ],
   "source": [
    "# Save the meshes needed for the remapped dataset\n",
    "from utils.partnet_utils import class_to_shapenet_dir, shapeid_to_partnet_dir\n",
    "o3d.utility.set_verbosity_level(o3d.utility.VerbosityLevel.Error)\n",
    "SHAPENET_PATH = '/home/sjauhri/IAS_WS/ShapeNetCore.v2'\n",
    "# PARTNET_PATH  = '/home/sjauhri/IAS_WS/PartNet/data_v0'\n",
    "PARTNET_PATH  = '/home/sjauhri/IAS_WS/potato-net/affordance/3DAffordanceNet/data_root/3DGraspAff/obj_meshes/'+train_or_test+'/PartNet_full'\n",
    "\n",
    "SHAPENET_SAVE_PATH = data_root + '/obj_meshes/'+train_or_test+'/ShapeNet'\n",
    "# PARTNET_SAVE_PATH  = data_root + '/obj_meshes/'+train_or_test+'/PartNet_full'\n",
    "PARTNET_SAVE_PATH  = data_root + '/obj_meshes/'+train_or_test+'/PartNet'\n",
    "\n",
    "tighten_decimate_mesh = True\n",
    "tighten_resolution = 20_000\n",
    "\n",
    "# scaling_parameters\n",
    "shapenet_scalings = {\n",
    "    'Knife': 0.22, #0.18\n",
    "    'Bag': 0.175,\n",
    "    'Bottle': 0.18,\n",
    "    # 'Scissors': 0.165, # normally no scissors\n",
    "    'Mug': 0.12,\n",
    "    'Earphone': 0.16,\n",
    "    'Hat': 0.175,\n",
    "    }\n",
    "partnet_scalings = {\n",
    "    # 'Knife': 0.165, # normally no knives\n",
    "    'Bag': 0.065,\n",
    "    # 'Bottle': 0.165, # normally no bottles\n",
    "    'Scissors': 0.12, # 0.075\n",
    "    # 'Mug': 0.11, # normally no mugs\n",
    "    'Earphone': 0.07,\n",
    "    'Hat': 0.07,\n",
    "}\n",
    "\n",
    "# loop through the dataset\n",
    "for sem_classs in aff_dataset['semantic_classes']:\n",
    "    shape_ids = list(aff_dataset['data'][sem_classs].keys())\n",
    "    start_id = 0\n",
    "    end_id =  len(shape_ids) # min(200, len(shape_ids))\n",
    "    # rand_id = random.randint(start_id, end_id)\n",
    "    # for shape_id in shape_ids[rand_id:rand_id+1]:\n",
    "    for shape_id in shape_ids[start_id:end_id]:\n",
    "        print(\"Processing shape_id: \", shape_id, \" for class: \", sem_classs)\n",
    "        shapeid_path = opj(SHAPENET_PATH, class_to_shapenet_dir[sem_classs.lower()],\n",
    "                        shape_id)\n",
    "        obj_path = opj(shapeid_path, 'models', 'model_normalized.obj')\n",
    "        # check if obj_path exists\n",
    "        if os.path.exists(obj_path):\n",
    "            # try load with open3d\n",
    "            obj_mesh = o3d.io.read_triangle_mesh(obj_path)\n",
    "            # if successful, just copy everything to the save path\n",
    "            if not os.path.exists(opj(SHAPENET_SAVE_PATH, shape_id)):\n",
    "                os.makedirs(opj(SHAPENET_SAVE_PATH, shape_id))\n",
    "            os.system('cp -r {} {}'.format(shapeid_path, SHAPENET_SAVE_PATH))\n",
    "\n",
    "            # Scale the stored pointcloud to match the original shapenet mesh\n",
    "            datapoint = aff_dataset['data'][sem_classs][shape_id]\n",
    "            pcl = datapoint['coordinates']\n",
    "            orig_points = obj_mesh.sample_points_uniformly(number_of_points=8000).points\n",
    "            # get the center of the mesh\n",
    "            orig_center = np.mean(orig_points, axis=0)\n",
    "            # get the maximum extent of the mesh\n",
    "            centered_points = orig_points - orig_center # center the points\n",
    "            orig_max_dist = np.max(np.sqrt(np.sum(centered_points**2, axis=1)))\n",
    "            # correct the pcl\n",
    "            corrected_pcl = pcl\n",
    "            # # flip along the z axis??\n",
    "            corrected_pcl[:,2] = -corrected_pcl[:,2]\n",
    "            # # flip along the x axis??\n",
    "            corrected_pcl[:,0] = -corrected_pcl[:,0]\n",
    "            # scale the point cloud\n",
    "            corrected_pcl *= orig_max_dist\n",
    "            # recenter the point cloud\n",
    "            corrected_pcl += orig_center\n",
    "\n",
    "            # scale the mesh and the point cloud and save\n",
    "            vertices = np.asarray(obj_mesh.vertices)\n",
    "            vertices *= shapenet_scalings[sem_classs]\n",
    "            corrected_pcl *= shapenet_scalings[sem_classs]\n",
    "            # DEBUG.....\n",
    "            # break\n",
    "            # .....DEBUG\n",
    "            if tighten_decimate_mesh:\n",
    "                # optional: make mesh watertight\n",
    "                obj_mesh = make_watertight_pcu(vertices, np.asarray(obj_mesh.triangles), resolution=tighten_resolution)\n",
    "            # save mesh\n",
    "            o3d.io.write_triangle_mesh(opj(SHAPENET_SAVE_PATH, shape_id, 'models', 'model_normalized.obj'), obj_mesh)\n",
    "        else:\n",
    "            print(\"ShapeNet category not found, try PartNet?\")\n",
    "            partnet_anno_id = shapeid_to_partnet_dir[shape_id]\n",
    "            \n",
    "            mesh_path = opj(PARTNET_PATH, partnet_anno_id, 'model_normalized.obj')\n",
    "            obj_mesh = o3d.io.read_triangle_mesh(mesh_path)\n",
    "            # scale the mesh and the point cloud and save\n",
    "            datapoint = aff_dataset['data'][sem_classs][shape_id]\n",
    "            pcl = datapoint['coordinates']\n",
    "            vertices = np.asarray(obj_mesh.vertices)\n",
    "            vertices *= partnet_scalings[sem_classs]\n",
    "            corrected_pcl = pcl\n",
    "            corrected_pcl *= partnet_scalings[sem_classs]\n",
    "\n",
    "            # if using original PartNet meshes\n",
    "            # meshes_path = opj(PARTNET_PATH, partnet_anno_id, 'objs')\n",
    "            # # loop through all meshes in the folder and add to open3d object\n",
    "            # obj_mesh = o3d.geometry.TriangleMesh()\n",
    "            # for mesh in os.listdir(meshes_path):\n",
    "            #     # optional: only load files with 'original' in the name\n",
    "            #     mesh_path = opj(meshes_path, mesh)\n",
    "            #     mesh = o3d.io.read_triangle_mesh(mesh_path)\n",
    "            #     obj_mesh += mesh\n",
    "            \n",
    "            if tighten_decimate_mesh:\n",
    "                # optional: make mesh watertight\n",
    "                obj_mesh = make_watertight_pcu(vertices, np.asarray(obj_mesh.triangles), resolution=tighten_resolution)\n",
    "            # save\n",
    "            if not os.path.exists(opj(PARTNET_SAVE_PATH, shape_id, partnet_anno_id)):\n",
    "                os.makedirs(opj(PARTNET_SAVE_PATH, shape_id, partnet_anno_id))\n",
    "            save_path = opj(PARTNET_SAVE_PATH, shape_id, partnet_anno_id, 'model_normalized.obj')\n",
    "            o3d.io.write_triangle_mesh(save_path, obj_mesh, write_triangle_uvs=True)\n",
    "            # DEBUG.....\n",
    "            # break\n",
    "            # .....DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DEBUG.....\n",
    "# # print min, max xyz of the corrected pcl\n",
    "# print(\"Min xyz: \", np.min(corrected_pcl, axis=0))\n",
    "# print(\"Max xyz: \", np.max(corrected_pcl, axis=0))\n",
    "# soccer_ball = o3d.io.read_triangle_mesh('/home/sjauhri/IAS_WS/potato-net/GIGA-TSDF/GIGA-6DoF/data/urdfs/pile/test/soccer_ball_poisson_000_visual.obj')\n",
    "# # paint\n",
    "# soccer_ball.paint_uniform_color([1, 0, 0])\n",
    "# draw([obj_mesh, o3d.geometry.PointCloud(o3d.utility.Vector3dVector(corrected_pcl)), soccer_ball])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save updated dataset\n",
    "new_aff_path = opj(data_root, 'scaled_anntd_remapped_full_shape_'+train_or_test+'_data.pkl')\n",
    "with open(new_aff_path, 'wb') as f:\n",
    "    pkl.dump(aff_dataset, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
