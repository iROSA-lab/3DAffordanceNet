{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join as opj\n",
    "import numpy as np\n",
    "\n",
    "import open3d as o3d\n",
    "from open3d.web_visualizer import draw\n",
    "\n",
    "from utils import *\n",
    "from dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 2285 samples\n"
     ]
    }
   ],
   "source": [
    "# set_random_seed(0)\n",
    "\n",
    "data = dict(\n",
    "    data_root='data_root',\n",
    "    partial=False,\n",
    "    category=['grasp', 'contain', 'lift', 'openable', 'layable', 'sittable',\n",
    "              'support', 'wrap_grasp', 'pourable', 'move', 'displaY', 'pushable', 'pull',\n",
    "              'listen', 'wear', 'press', 'cut', 'stab']\n",
    ")\n",
    "\n",
    "data_root = data['data_root']\n",
    "partial = data['partial']\n",
    "aff_dataset = AffordNetDataset(data_root, 'train', partial=partial)\n",
    "# aff_dataset = AffordNetDataset(data_root, 'val', partial=partial)\n",
    "print(\"Loaded dataset with {} samples\".format(len(aff_dataset)))\n",
    "\n",
    "# import pickle as pkl\n",
    "# with open(opj('data_root', 'full_shape_%s_data.pkl' % 'train'), 'rb') as f:\n",
    "#     temp_data = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_of_interest = ['Knife', 'Bag', 'Bottle', 'Scissors', 'Mug', 'Earphone', 'Hat']\n",
    "\n",
    "# class -> affordance remapping beacuse we want it agent-centric\n",
    "aff_remaps = {\n",
    "    \"Knife\": {\n",
    "        \"cut\": [\"handover\"],\n",
    "        \"stab\": [\"handover\"],\n",
    "        \"grasp\": [\"cut\", \"stab\"],\n",
    "        },\n",
    "    \"Bag\": {\n",
    "        \"lift\": [\"lift\"],\n",
    "        \"grasp\": [\"handover\"],\n",
    "        },\n",
    "    \"Bottle\": {\n",
    "        \"openable\": [\"handover\", \"wrap\"],\n",
    "        \"pourable\": [\"handover\", \"wrap\"],\n",
    "        \"wrap_grasp\": [\"pour\"],\n",
    "        },\n",
    "    \"Scissors\": {\n",
    "        \"cut\": [\"handover\"],\n",
    "        \"stab\": [\"handover\"],\n",
    "        \"grasp\": [\"cut\", \"stab\"],\n",
    "        },\n",
    "    \"Mug\": {\n",
    "        \"wrap_grasp\": [\"handover\"],\n",
    "        \"grasp\": [\"wrap\", \"pour\"],\n",
    "        },\n",
    "    \"Earphone\": {\n",
    "        \"listen\": [\"handover\"],\n",
    "        \"grasp\": [\"lift\"],\n",
    "        },\n",
    "    \"Hat\": {\n",
    "        \"wear\": [\"handover\"],\n",
    "        \"grasp\": [\"wear\"],\n",
    "        },\n",
    "}\n",
    "# Remember to update this:\n",
    "affordance_classes = ['handover', 'cut', 'stab', 'lift', 'wrap', 'pour', 'wear']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No non-zero scores for this datapoint\n"
     ]
    }
   ],
   "source": [
    "# Initialize new dataset\n",
    "new_aff_dataset = {\n",
    "    'semantic_classes': classes_of_interest,\n",
    "    'affordance_classes': affordance_classes,\n",
    "    'num_objects': 0,\n",
    "    'data': dict()\n",
    "}\n",
    "# initialize sem_class keys\n",
    "for sem_class in classes_of_interest:\n",
    "    new_aff_dataset['data'][sem_class] = dict()\n",
    "\n",
    "\n",
    "# Loop through the whole dataset and remap affordances of interest\n",
    "for i, datapoint in enumerate(aff_dataset.all_data):\n",
    "    if datapoint['semantic class'] not in classes_of_interest:\n",
    "        continue\n",
    "\n",
    "    sem_class = datapoint['semantic class']\n",
    "    shape_id = datapoint['shape_id']\n",
    "    # print(\"Shape id \", shape_id, \"\\nSemantic class: \", sem_class)\n",
    "\n",
    "    new_datapoint = {\n",
    "                'coordinates': np.array([], dtype=np.float32),\n",
    "                'labels': dict()\n",
    "                }\n",
    "\n",
    "    # Add pcl coordinates to new datapoint\n",
    "    new_datapoint['coordinates'] = datapoint['data_info']['coordinate']\n",
    "    # Get labels of interest for this sem class\n",
    "    labels_of_interest = aff_remaps[sem_class].keys()\n",
    "    # print(\"Labels of interest: \", labels_of_interest)\n",
    "    non_zero_score = False\n",
    "    for key in labels_of_interest:\n",
    "        labels = datapoint['data_info']['label'][key]\n",
    "        score = labels.sum()\n",
    "        # print(\"Old Label: \", key, \"Score: \", score)\n",
    "\n",
    "        # remap and add to new datapoint if score is above zero\n",
    "        if score > 0.0:\n",
    "            non_zero_score = True\n",
    "            # new labels can be more than one (see aff_remaps) so loop through them and add to the datapoint\n",
    "            new_keys_list = aff_remaps[sem_class][key]\n",
    "            for new_key in new_keys_list:\n",
    "                # if new key already exists in the new datapoint, add the labels to the existing labels (OR operation)\n",
    "                if new_key in new_datapoint['labels']:\n",
    "                    new_datapoint['labels'][new_key] = np.logical_or(new_datapoint['labels'][new_key], labels)\n",
    "                else:\n",
    "                    new_datapoint['labels'][new_key] = labels\n",
    "                # print(\"New Label: \", new_key)\n",
    "\n",
    "    # Add the new datapoint to the new dataset if there is at least one non-zero score\n",
    "    if non_zero_score == True:\n",
    "        new_aff_dataset['data'][sem_class][shape_id] = new_datapoint\n",
    "        new_aff_dataset['num_objects'] += 1\n",
    "    else:\n",
    "        print(\"No non-zero scores for this datapoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of shapes in new training dataset:  152\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of shapes in new training dataset: \", new_aff_dataset['num_objects'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save the new dataset\n",
    "import pickle as pkl\n",
    "with open(opj('data_root', 'remapped_full_shape_%s_data.pkl' % 'train'), 'wb') as f:\n",
    "    pkl.dump(new_aff_dataset, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
